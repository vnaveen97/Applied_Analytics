---
title: 'STAT656: Homework 2'
subtitle: Some Topics on Multiple Linear Regression 
output:
  html_document: default
---

Let's load in any required packages here
```{r loadingPackages}
require(dplyr)
require(caret)
require(doParallel)
```

We should have our saved R object '2019flights.Rdata'.  Either change the pathname in the load statement or copy the file to the directory this Rmd file is in.

```{r loadData}
load('2019flights.Rdata')
flightsNotCancelled = df %>%
  filter(CANCELLED == 0) %>% 
  na.omit
```

In order to speed up some computations for the purposes of this homework, let's go ahead and subsample from the entire database of flights.  Note that this is a typical path taken in industry, particularly when developing new products/methods. Lastly, it is a good practice to clean up large objects in R's memory if you are not going to reference them again (or have them available on the hard drive to load in later).

```{r subsample}
set.seed(1)
nTotal = nrow(flightsNotCancelled)
flightsNotCancelled = flightsNotCancelled[sample(1:nTotal,round(nTotal/2)),]

# Let's clean up the name space to free up memory
rm(df)
```

# Problem 1. The bootstrap

Suppose we want to estimate the difference in the mean 'arrival delay' of flights between the winter and the summer.  **Let's define October through March as the winter and April through September the summer.**


## 1.1. Using normality assumption 
Here, we will apply the classic "pooled two sample t-test" for testing the population means between two groups.  We won't go over this test explicitly in this course.  I'm presuming you have seen it before in one of your earlier classes. If you haven't seen it before, now is a good time!

### 1.1.1. Getting the difference in sample means
**Get the sample means for these two groups, the sample variances for each group, and the sample size for each group**

```{r problem11}
winter = flightsNotCancelled %>% 
  filter(MONTH >= 10 | MONTH <= 3) %>% 
  summarise(mean = mean(ARR_DELAY), var = var(ARR_DELAY), n = n())
summer = flightsNotCancelled %>% 
  filter(MONTH >= 4 | MONTH <= 9) %>% 
  summarise(mean = mean(ARR_DELAY), var = var(ARR_DELAY), n = n())#### Answer 1.1.1.1
```

#### Answer 1.1.1.2
The sample mean difference between winter and summer is `r winter$mean - summer$mean`

### 1.1.2. Getting the pooled variance

Getting the pooled sample variance and the quantile from the t-distribution, we can get the pooled two-sample t-test confidence interval
```{r problem11ttest}
winterMinusSummer   = winter$mean - summer$mean
n                   = winter$n + summer$n
pooledVar           = ((winter$n - 1)*winter$var + (summer$n - 1)*summer$var)/(n - 2)
tQuantile           = qt(1-0.05/2,n-2)

winterMinusSummerSE = sqrt(pooledVar)*sqrt(1/winter$n + 1/summer$n)
normalCI            = c(winterMinusSummer - tQuantile * winterMinusSummerSE,
                        winterMinusSummer + tQuantile * winterMinusSummerSE)
```

#### Answer 1.1.2.1

We have chosen the 95% confidence interval and the end points of this interval are [`r normalCI[1]`, `r normalCI[2]`]. We have used the mean difference between winter and summer as the population parameter.

**Provide an interpretation of this confidence interval.** To interpret a confidence interval, state the level of confidence, the end points of the interval, and what population parameter we are estimating with that interval.

## 1.2. Using bootstrap

Let's do 10 bootstrap draws and compare the confidence interval (we should do a lot more, but this is a HW afterall).  We could use caret, but that allocates all the bootstrap draws at once, which takes a lot of memory for this problem.  We can use "sample' instead.  Note that with this many observations, we won't need to worry about stratified sampling over season.

```{r bootstrapSequential, cache = TRUE}
nBootstrapDraws = 10#### Answer 1.2.1 Note, I recommend setting this to 1 or 2  until you decide you've got the code right.  Then use the requested number

bootstrapResults = rep(0,nBootstrapDraws)
srt = proc.time()[3]
for(b in 1:nBootstrapDraws){
  flightsNotCancelled_boot = flightsNotCancelled[sample(1:n,n,replace=TRUE), ]  
  winter = flightsNotCancelled_boot %>% 
  filter(MONTH >= 10 | MONTH <= 3) %>% 
  summarise(mean = mean(ARR_DELAY))
summer = flightsNotCancelled_boot %>% 
  filter(MONTH >= 4 | MONTH <= 9) %>% 
  summarise(mean = mean(ARR_DELAY))##### Answer 1.2.2 compute the two sample means
  
  bootstrapResults[b] = winter$mean - summer$mean#### Answer 1.2.3 compute the sample mean difference
}
end = proc.time()[3]

lowerQuantile = quantile(bootstrapResults,prob = 0.025)
upperQuantile = quantile(bootstrapResults,prob = 0.975)
bootstrapCI   = c(lowerQuantile,upperQuantile)#### Answer 1.2.4. 
              

#again, let's clean up a bit to keep our memory fresh(er)
rm(flightsNotCancelled_boot)
```

A 95% bootstrap confidence interval would be (`r bootstrapCI[1]`, `r bootstrapCI[2]`).  This took `r end - srt` seconds to compute.

Now, let's parallelize this so that it will run faster

```{r ncores}
nCores = detectCores()
nCores
```
This system has `r nCores` cores.  However, the data set takes up a fair amount of memory

```{r memory}
format(object.size( flightsNotCancelled ),units='MB')
```

If you have a lot of cores on your system, **you might want to limit the cluster to 3 (If you have fewer than 3 cores, then just choose that many).** Note that I have to re-require(dplyr) inside the parallel call.  This is due to the fact that a new R session is created at each core.

```{r bootstrapParallel, cache = TRUE}
cl = makeCluster(3)
registerDoParallel(cl)

bootstrapResults = rep(0,nBootstrapDraws)

srt = proc.time()[3]
foreach(b = 1:nBootstrapDraws) %dopar%{#### Answer 1.2.5 put the parallel for loop call here
  require(dplyr)
  flightsNotCancelled_boot = flightsNotCancelled[sample(1:n,n,replace=TRUE), ]  
  winter = flightsNotCancelled_boot %>% 
    filter(MONTH >= 10 | MONTH <= 3) %>% 
    summarise(mean = mean(ARR_DELAY))
  summer = flightsNotCancelled_boot %>% 
    filter(MONTH >= 4 | MONTH <= 9) %>% 
    summarise(mean = mean(ARR_DELAY))#### Answer 1.2.6
  
  bootstrapResults[b] = winter$mean - summer$mean
}
end = proc.time()[3]

stopCluster(cl)
registerDoSEQ()# This returns the session to "serial" instead of "parallel"
```

This took `r end - srt` seconds to compute.

# Problem 2. Measuring performance in regression

Let's process the data similarly to last time, keeping DEP_DELAY and ARR_DELAY

```{r problem2}
rm(list=ls())# Just to clean up the memory again

load('2019flights.Rdata')
flightsNotCancelled = df %>%
  filter(CANCELLED == 0) %>%
  select(ARR_DELAY,DEP_DELAY) %>%
  na.omit()

rm(df)
```

## 2.1 Training/Validation/Test Split

Let's get a training/validation/test split for evaluating our regression model's performance.  Split the data into 50% train and 25% validation and 25% test.  **Follow the code and object naming convention in caretPackage.rmd**

```{r problem21split}
trainIndex       = createDataPartition(flightsNotCancelled$ARR_DELAY, p = .5, list = FALSE) %>% as.vector(.)
validSplit       = createDataPartition(flightsNotCancelled$ARR_DELAY[-trainIndex], p = .5, list = FALSE) %>% as.vector(.)
n                = nrow(flightsNotCancelled)
testIndex        = (1:n)[-trainIndex][-validSplit]
validIndex       = (1:n)[-trainIndex][validSplit]

role             = rep('train',n)
role[testIndex]  = 'test'
role[validIndex] = 'validation'
#### Answer 2.1.1
```

## 2.2. Do early departures matter?

We want to start to think about choosing features in a principled manner. Let's compare
using the raw DEP_DELAY to DEP_DELAY with the negative numbers truncated at zero, we will call DEP_DELAY_TRUNC. **First, we created a new variable in the exsiting dataframe.**

```{r}
flightsNotCancelled      = flightsNotCancelled %>% 
  mutate(DEP_DELAY_TRUNC = if_else(DEP_DELAY< 0,0,DEP_DELAY)) #### Answer 2.2.1
```

The fundamental idea behind data splitting is to not use the same data for multiple purposes, which could lead to use getting overly optimistic results.

Hence, we want to do the following:
 * compute the both models using the training data
 * choose which model you prefer using the validation data by comparing the squared error
 * then use the test data to get a good estimate of the test error.
 
### Training data

```{r problem22training}
Xtrain      = data.frame(X = flightsNotCancelled$DEP_DELAY[role == 'train'])
XtruncTrain = data.frame(X = flightsNotCancelled$DEP_DELAY_TRUNC[role == 'train'])
Ytrain      = flightsNotCancelled$ARR_DELAY[role == 'train']

lmOut       = lm(Ytrain ~ ., data = Xtrain)
lmTruncOut  = lm(Ytrain ~ ., data = XtruncTrain)
```

### Validation data

```{r problem22validation}
Xvalid       = data.frame(X = flightsNotCancelled$DEP_DELAY[role == 'validation'])
XtruncValid  = data.frame(X = flightsNotCancelled$DEP_DELAY_TRUNC[role == 'validation'])### Answer 2.2.2
Yvalid       = flightsNotCancelled$ARR_DELAY[role == 'validation']

Yhat         = predict.lm(lmOut, Xvalid)
YtruncHat    = predict.lm(lmTruncOut, XtruncValid)### Answer 2.2.3

validSqError = list('original' = sum( (Yhat - Yvalid)**2 ),
                    'truncated' = sum( (YtruncHat - Yvalid)**2 ))
validSqError
```

#### Answer 2.2.4 

Is the original or truncated feature better?  

Original feature does better than Truncated feature as the squared error is low for Original. So early departure matters in this case!.

#### Answer 2.2.5
We just used the validation data for an important purpose.  **Explain what that purpose is and why we should appeal to test data now for getting a fair estimate of the test error (hint: imagine we just compared thousands of models to each other instead of just 2)**

The purpose of validation data is used to tune the models using the training data and then choose the best, comparing the validation squared error with the help of validation data. Validation data helps us to understand how the model behaves when new data is introduced. 

### 2.2.1 Test data

Now, we can get a fair estimate of the test error 
```{r problem22test}
Xtest       = data.frame(X = flightsNotCancelled$DEP_DELAY[role == 'test'])
Ytest       = flightsNotCancelled$ARR_DELAY[role == 'test']

Yhat        = predict.lm(lmOut, Xtest)

testSqError = list('original' = sum( (Yhat - Ytest)**2 ))#
```

#### Answer 2.2.6

What is the estimate of the test squared error using the test data?

It is `r testSqError`


## Problem 3. Extra Credit

We will be computing the coefficient of determination (i.e. R squared) for both these features and using the training/validation/test sets for different purposes.

Analogously to the previous problem, compute the both models using the training data, choose which model you prefer using the validation data, and then use the test data to get a good estimate of the true R squared.

```{r training3, cache = TRUE}
#For training
Xtrain      = data.frame(X = flightsNotCancelled$DEP_DELAY[role == 'train'])
XtruncTrain = data.frame(X = flightsNotCancelled$DEP_DELAY_TRUNC[role == 'train'])
Ytrain      = flightsNotCancelled$ARR_DELAY[role == 'train']

lmOut       = lm(Ytrain ~ ., data = Xtrain)
lmTruncOut  = lm(Ytrain ~ ., data = XtruncTrain)
```


```{r R-squaredValidation, cache = TRUE}
#For Validation
Xvalid        = data.frame(X = flightsNotCancelled$DEP_DELAY[role == 'validation'])
XtruncValid   = data.frame(X = flightsNotCancelled$DEP_DELAY_TRUNC[role == 'validation'])
Yvalid        = flightsNotCancelled$ARR_DELAY[role == 'validation']

Yhat          = predict.lm(lmOut, Xvalid)
YtruncHat     = predict.lm(lmTruncOut, XtruncValid)

SSEvalid      = sum((Yvalid - Yhat)**2)
TSSvalid      = sum((Yvalid - mean(Yvalid))**2)
r2Original    = 1 - SSEvalid/TSSvalid

SSEvalidTrunc = sum((Yvalid - YtruncHat)**2)
TSSvalidTrunc = sum((Yvalid - mean(Yvalid))**2)
r2validTrunc  = 1 - SSEvalidTrunc/TSSvalidTrunc
```

The R-squared value for original and truncated models are `r r2Original` & `r r2validTrunc`. Looking at the values for the R-squared, the original model explains the variance better. 

```{r R-squared test, cache = TRUE}
#For testing
Xtest       = data.frame(X = flightsNotCancelled$DEP_DELAY[role == 'test'])
Ytest       = flightsNotCancelled$ARR_DELAY[role == 'test']

Yhat        = predict.lm(lmOut, Xtest)

SSEtest = sum((Ytest - Yhat)**2)
TSStest = sum((Ytest - mean(Ytest))**2)
r2test = 1 - SSEtest/TSStest
```

The R-squared value for the test data is found to be `r r2test`.

